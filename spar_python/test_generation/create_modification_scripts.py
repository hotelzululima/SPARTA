# *****************************************************************
#  Copyright 2013 MIT Lincoln Laboratory  
#  Project:            SPAR
#  Authors:            Jill
#  Description:        Generate modification tests
# *****************************************************************


import csv
import sys
import os
from optparse import OptionParser
import logging
import collections
import random
this_dir = os.path.dirname(os.path.realpath(__file__))
base_dir = os.path.join(this_dir, '..', '..')
sys.path.append(base_dir)
import spar_python.report_generation.ta1.ta1_schema as rdb
import spar_python.report_generation.ta1.ta1_database as ta1_database
import spar_python.query_generation.query_schema as qs
import spar_python.test_generation.query_file_handler as qfh
import spar_python.test_generation.test_utils as tu
import spar_python.test_generation.modification_generator as mg

#import MySQLdb
#def get_largest_row_id(databases):
#    '''
#    open all the databases and get the largest row id,
#    return the largest value from all the databases
#    '''
#    max_row_id = 0
#    for db_name in databases:        
#
#        db = MySQLdb.connect(host="localhost",
#                             user="root",
#                             passwd="SPARtest",
#                             db=db_name)
#        cur = db.cursor() 
#        cur.execute("SELECT MAX(id) FROM main")
#
#        # expect a list of 1 element containing a tuple of which we want
#        # the first element which should be the max row id as a string
#        rows = cur.fetchall()
#        if len(rows) != 1:
#            logger.error("Unable to get max row id for db=%s" % db_name)
#        if len(rows[0]) < 1:
#            logger.error("Unable to get max row id for db=%s" % db_name)
#        val = long(rows[0][0])
#        max_row_id = max(max_row_id, val)
#
#        cur.close()
#        db.close()
#
#    assert max_row_id > 0
#    return max_row_id

def main():
    '''
    Generate all the modification test scripts
    '''


    #
    # Specify options
    #

    parser = OptionParser()
    parser.add_option('-r', '--result_databases',
                      type='string',
                      action='callback',
                      callback=tu.callback_for_list_options,
                      help='List of paths to the result databases to process. '
                      'Syntax: --result_databases \"../10g/10grdb.db,'
                      '../1g/1grdb.db,../100g/100grdb.db\"  '
                      'There should be a one to one correspondence between the list of databases, the list of result_databases, list of schemas, and the list of extra row line raw files. Each list must be the same length and in the same order. For example, the first element in each list should be for the same database size.')

    #parser.add_option('-d', '--databases',
    #                  type='string',
    #                  action='callback',
    #                  callback=tu.callback_for_list_options,
    #                  help='List of paths to the databases (generated by '
    #                  'data_gen) to process. It uses these databases to '
    #                  'determine the largest row ID and does not modify them.'
    #                  'Syntax: --databases \"../10g/10g_100kBpr.db,'
    #                  '../1g/1g_100kBpr.db,../100g/100g_100kBpr.db\"  '
    #                  'There should be a one to one correspondence between the list of databases, the list of result_databases, list of schemas, and the list of extra row line raw files. Each list must be the same length and in the same order. For example, the first element in each list should be for the same database size.')

    parser.add_option('-l', '--line_raw_files',
                      type='string',
                      action='callback',
                      callback=tu.callback_for_list_options,
                      help='List of paths to the lineraw files containing extra rows. '
                      'Syntax: --line_raw_files \"../10g/rows.lr,'
                      '../1g/rows.lr,../100g/rows.lr\"  '
                      'There should be a one to one correspondence between the list of databases, the list of result_databases, list of schemas, and the list of extra row line raw files. Each list must be the same length and in the same order. For example, the first element in each list should be for the same database size.')
    parser.add_option('-s', '--schema_files',
                      type='string',
                      action='callback',
                      callback=tu.callback_for_list_options,
                      help='List of paths to the schema files to process. '
                      'Syntax: --schema_files \"../10g/schema.csv,'
                      '../1g/schema.csv,../100g/schema.csv\"  '
                      'There should be a one to one correspondence between the list of databases, the list of result_databases, list of schemas, and the list of extra row line raw files. Each list must be the same length and in the same order. For example, the first element in each list should be for the same database size.')
    parser.add_option("-v", '--verbose', dest="verbose",
                      action="store_true", default=False,
                      help="Verbose output")
    parser.add_option("-o", "--output_dir", dest="output_dir",
                      default="./Tests",
                      help="Base directory where test files will be generated")
    parser.add_option("-n", "--number_of_tests", dest='number_of_tests', 
                      type = 'int', default = 10,
                      help="Number of tests of each type to generate,"
                      " the default is 10")
    parser.add_option("-m", "--max_db_row_id", dest='max_db_row_id', 
                      type = 'int',
                      help="Specify the largest row id in all the databases")
    (options, args) = parser.parse_args()


    #
    # Create logger
    #

    if options.verbose:
        log_level = logging.DEBUG
    else:
        log_level = logging.WARNING
    logging.basicConfig(
        level = log_level, format = '%(filename)s: %(levelname)s: %(message)s')
    logger = logging.getLogger(__name__)


    # 
    # Check options
    #

    # --output_dir
    if not options.output_dir:
        logger.error("No output directory specified. Specify --output_dir")
        sys.exit(1)
    options.output_dir = tu.make_expanded_path(options.output_dir)
    # make the directory if it does not exist
    if not os.path.exists(options.output_dir):
        os.makedirs(options.output_dir)

    # --line_raw_files
    if not options.line_raw_files:
        logger.error("No lineraw file specified.  Specify --line_raw_files")
        sys.exit(1)
    # expand the path of all the lineraw files
    expanded_paths = []
    for lr in options.line_raw_files:
        exp_path = tu.make_expanded_path(lr)
        if not os.path.isfile(lr):
            logger.error('lineraw file ' + exp_path + ' does not exist')
            sys.exit(1)
        expanded_paths.append(exp_path)
    options.line_raw_files = expanded_paths
    
    # --schema-files
    if not options.schema_files:
        logger.error("No schema files specified.  Specify --schema_files")
        sys.exit(1)
    # expand the path of all the lineraw files
    expanded_paths = []
    for schema in options.schema_files:
        exp_path = tu.make_expanded_path(schema)
        if not os.path.isfile(schema):
            logger.error('schema file ' + exp_path + ' does not exist')
            sys.exit(1)
        expanded_paths.append(exp_path)
    options.schema_files = expanded_paths
    
    ## --databases
    #if not options.databases:
    #    logger.error("No databases specified. Specify --databases")
    #    sys.exit(1)
 
    # --result_databases
    if not options.result_databases:
        logger.error("No result databases specified. Specify --result_databases")
        sys.exit(1)
    # expand the path of all the db files
    expanded_paths = []
    for resultdb in options.result_databases:
        exp_path = tu.make_expanded_path(resultdb)
        if not os.path.isfile(exp_path):
            logger.error('result database ' + exp_path + ' does not exist')
            sys.exit(1)
        expanded_paths.append(exp_path)
    options.result_databases = expanded_paths
  
    
    num_result_databases = len(options.result_databases)
    if len(options.schema_files) != num_result_databases or \
       len(options.line_raw_files) != num_result_databases:
        logger.error('There should be a one to one correspondence between the list of databases, the list of result_databases, list of schemas, and the list of extra row line raw files. Each list must be the same length and in the same order. For example, the first element in each list should be for the same database size.')
        sys.exit(1)


    # at this point the options look good


    # Determine the largest row id in any of the databases
    #max_row_id = get_largest_row_id(options.databases)
    max_row_id = options.max_db_row_id
    max_row_id = max_row_id + 1
    print "MAX ID=",max_row_id

    # Generate tests for each database
    for lineraw_file, schema_file, resultdb_name in \
            zip(options.line_raw_files, options.schema_files, 
                options.result_databases): 

        resultdb = ta1_database.Ta1ResultsDB(resultdb_name)

        # make a performer/dbsize directory under output_dir
        (db_record_size, db_num_records) = tu.get_db_size(resultdb)
        dbsize_str = tu.get_dbsize_string(db_record_size, db_num_records)
        dbsize_dir = os.path.join(options.output_dir, "COMMON", dbsize_str)
        if not os.path.exists(dbsize_dir):
            os.makedirs(dbsize_dir)

        doing_select_star = True # TODO Support this
        mod_generator = mg.ModificationGenerator(max_row_id, resultdb, 
                                              lineraw_file, schema_file,
                                              dbsize_dir, doing_select_star)
        mod_generator.create_tests(options.number_of_tests)

        resultdb.close()



if __name__ == "__main__":
    main()
