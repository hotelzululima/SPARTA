

Lincoln Laboratory's data-generation system uses demographic
information from the United States Census Bureau and other sources
to generate unlimited amounts of synthetic, random database entries 
about fictitious people. It can also, while generating this data,
generate a set of SQL queries and ground-truth answers to those
queries over the data generated. In this document, we will describe
how to use this system.


Table of contents
--------------

* Quick start
* Changes from previous versions
* Expedited execution
* Command-line flags
* Query generation


==============================================

"QUICK" START
------------

As opposed to previous versions of this software, you must retrieve
the demographic source-data used to train our system. This is an
expensive but one-time operation:

1) Install our software and (see the installation instructions) and invoke
   the relevant virtualenv.

2) Navigate to the directory <toplevel>/spar_python/data_generation/learning

3) Execute the following command:

  $ python get_data.py -d path/to/desired/data/directory

  (The option -v flag will make the output more verbose.)

The get_data.py script will retrieve approximately 5.8GB of data from
the web (and copy one data file distributed with our system).



After retrieving this demographic data, synthetic database entries can
be generated at any time. To generate 1,000 random entries/rows and
write them to a file:

1) If necessary, invoke the relevant virtualenv.

2) Navigate to the directory<toplevel>/spar_python/new_query_generation 

3) Execute the command

    $ python pre_test_generation.py -d /path/to/data/directory
    --line-raw-file=/path/to/desired/file
    --schema-file=<toplevel>scripts-config/ta1/config/metadata_schema.csv
    --num-rows=1000 --row_width=100


   As before, the -v flag can be used to produce more verbose output.


To generate large amounts of data, we recommend that you run more than
one process. To generate 1,000,000 rows using 10 processes, for
example, one should run the command:

    $ python pre_test_generation.py -d /path/to/data/directory
    --line-raw-file=/path/to/desired/file
    --schema-file=<toplevel>scripts-config/ta1/config/mysql_schema.csv
    --num-rows=1000000 --num-processes=10 --row_width=100

This will actually produce 10 files at the location specified, each
containing the PID of the generating process as a mechanism to enforce
unique filenames.





Changes from previous versions
--------------------------

* We no longer support CSV files or direct-connection to MySQL
  databases. The only output form supported are line-raw files. (We do
  still support named-pipes.) 

* The data-generator will re-learn demographic data from source-data
  with each execution, and will need the path to the source-data with
  each invocation. We no longer support the pre-digested 'pickle'
  mechanism.

* The data-generator now supports query-generation: the generation of
  SQL queries matching specific criteria, and the evaluation of those
  queries over the data generated. This is useful for testing.

* A temporary patch involves having the data generator wait indefinitely for
  subprocesses to complete their tasks. There is a possibility that the process
  will hang at this point; if you notice that the data generator has generated
  all the requested rows but is not terminating, do some sanity checking (i.e.
  check that your files look complete or that MariaDB has received all the
  requested rows) and terminate the process. Be aware that Ctrl-C'ing may also
  kill any parent process for the data generator; you may want to do a 'kill
  <pid>' instead to keep the parent process alive.




Expedited executions
------------------

We have found it useful to create a second source-data directory
(e.g., test_data/) that contains only a small subset of the
source-data. Although this will prevent the system from generating
correct data, it does run considerably faster and is useful for quick
tests of behavior. Specifically, we eliminated all PUMS data except
REVISESDPUMS5_28.TXT and was able to greatly speed the learning
process. When running in this mode, though, be sure to use the
--allow-missing-files flag to pre_test_generation.py.




Command-line flags
------------------

This section describes the options for pre_test_generation.py. 

  -h, --help
      Show a short help message and exit.

  -v, --verbose         
      Verbose output

   -d DATA_DIR, --data-dir=DATA_DIR
       Specify the location of the source-data files. Note: this
       should be the directory given to get_data.py, and will be the
       directory containing the directores 'pums', 'names', 'pums',
       etc.

   --allow-missing-files
        By default, the data-generator will terminate with error if
        any of the expected source-data files are missing. This flag
        suppresses this behavior, allowing data-generation to continue
        (if possible) on incomplete data. Though the generated data
        will be incorrect, this mode of operation may be useful for
        debugging or testing. (See 'Test-executions' above.)


   --line-raw-file=LINE_RAW_FILE
        Write LineRaw formatted data between INSERT/ENDINSERT pairs to
        this file.  Requires the presence of the --schema-file
        flag. Note: in multi-processor mode, will generate multiple
        files with names derived from the one specified
   
   --schema-file=SCHEMA_FILE
        Path to the schema file. We recommend against writing your
        own. Just use the one at spar-git/scripts-config/ta1/config. 

    --named-pipes       
         Use named pipes instead of regular files. This will *greatly
         speed up* data-generation, but the resulting 'files' will not
         be persistant.

   --seed=RANDOM_SEED  
        Random seed for generation. Must be a number, and defaults to
        0. Note: is converted to an integer via the python int()
        function before use, and so will be rounded down to the next
        integer. Numbers between 0 and 1, therefore, will all collapse
        to the default. We guarantee that re-use of the same seed will
        result in the same data, even if other options change between
        executions. (We do not guarantee that the data will
        be allocated to the processes or files, just that the final
        set of entries will be the same.)

    --num-processes=NUM_PROCESSES
          The number of processes to spawn when parallelization is
          desired. We suggest that you experiment with this parameter
          to maximize efficiency.

    --num-rows=NUM_ROWS
           The number of rows (fictitious people) to create.

    --row_width=ROW_WIDTH
           The average width of rows in the database, given in
           bytes


   --generate-queries  
   --query-generation-seed=QUERY_SEED
   --query-schema-file=Q_SCHEMA_FILE
   --result-database-file=RESULT_DATABASE_FILE
   --list-of-queries-file=LIST_OF_QUERIES_FILE

	These flags control the generation of sample
	queries/results. See the next section.

============================================================


Query generation
--------------

Query generation is an extension of data-generation: generating
queries according to user-specification, and computing the
ground-truth results of those queries over the generated data. The
desired queries are specified in an external 'query-schema'
file. During data-generation, these query-specifications are matched
against the underlying distribution of the source-data. Matching
queries are generated and matched against data during generation
(using a map-reduce mechanism). 

Because query-generation and data-generation are so linked, the
query-generator and data-generator are the same
system. Query-generation is added to data-generation using additional
command-line flags to pre_test_generation.py.


COMMAND LINE ARGUMENTS
----------------------

Query-specific options for spar_python/new_query_generation/pre_test_generation

--generate-queries  
	
	Toggle switch to generate queries on this run of data generation. 
	Defaults to false. This flag must be present if any of the other query 
	specific flags are used. 

--query-generation-seed=QUERY_SEED

      The seed for query generation. Defaults to 0. Note: is converted to 
      an integer via the python int() function before use, and so will 
      be rounded down to the next nteger. Numbers between 0 and 1, therefore, 
      will all collapse to the default. We guarantee that re-use of the same 
      seed will result in the same queries, if all other options remain constant.  
                    
--query-schema-file=Q_SCHEMA_FILE
    
    Path to the schema file for queries, this is used to determine which 
    queries need to be generated. Expanded instructions for the query schema
    file are included below. This file should be in .csv format. A full list 
    of the current master set of queries generated can be found under 
    Master_schema in query_generation directory of spar_python. 
    
    
--result-database-file=RESULT_DATABASE_FILE
    
    Path to the file where the ground truth database will be written to. This 
    generated file is in MySQL format and is used in the generation of the 
    final pdf report. After query generation is run, the following fields
    in the results/ground truth database are populated:
    	* Atomic: aqid, category, sub_catagory, db_num_records, db_record_size,
    	  where_clause, num_matching_records, field, field_type, keyword_len
    	  range
    	* Full: qid, category, sub_category, db_num_records, db_record_size,
    	  where_clause, num_matching_records, matching_record, ids, 
    	  matching_record_hashes
    	* Full to atomic: atomic_row_id, full_row_id
    
    
--list-of-queries-file=LIST_OF_QUERIES_FILE
    
    Path to the file which will be generated containing the list of queries 
    generated. They will be in the format ingestible by the test harness:
       
       (qid) SELECT * FROM main WHERE (where_clause)
        

Before each new run of data/query generation, the files indicated by
Q_SCHEMA_FILE and RESULT_DATABASE_FILE must not previously exist. 

What it looks like on the command line within spar_python/new_query_generation

 $ python pre_test_generation.py -d /path/to/data/directory
    --line-raw-file=/path/to/desired/file
    --schema-file=<toplevel>scripts-config/ta1/config/metadata_schema.csv
    --num-rows=10000 --generate-queries --query-schema=<QUERY_SCHEMA_FILE> 
    --result-database-file=results_database.sql --list-of-queries=queries.txt
    --row-width=100 --query-generation-seed=12 


QUERY SCHEMA FOR SUPPORTED QUERY TYPES
--------------------------------------

Included in the code drop is several example query_schema files for each type 
of supported query. The basic structure for the schema is

	*
	cat,sub_cat,perf,fields,"['no_queries','r_lower','r_upper']",
 	EQ,eq,"['LL','COL','IBM1']","['fname','lname']","[10,1,10]","[5,11,100]"
 	...(more queries)

The * indicates that the next row is a redefinition of the fields needed for 
that particular query type (outlined below for each particular query type). The
only thing that will ever change is what is contained in the other_columns list
when the fields are redefined.

The next line is the fields for the next n lines (until another * is hit or the 
end of the file is reached). It is important to have no blank lines in the 
schema.

Each subsequent n lines represents a set of queries. The cat columns, current 
valid values are:

	*EQ, P1, P2, P3, P4, P6, P7, P8, P9, P11  

Sub_cat valid fields are outlined within the specific categories below. Valid 
fields generally are (but are restricted below for different categories):

	*'fname'				*'citizenship'
	*'lname'				*'income'
	*'ssn'					*'military_service'
	*'zip'					*'language'
	*'city'					*'hours_worked_per_week'
	*'address'				*'weeks_worked_per_year'
	*'state'				*'foo'
	*'sex'					*'last_updated'
	*'age'					*'notes1'
	*'race'					*'notes2'
	*'marital_status'		        *'notes3'
	*'school_enrolled'	   	        *'notes4
	*'education'	     	               *'dob'

There can be as many fields as are supported within the list of fields
(as shown above with the listing of 'fname' and 'lname', realistically one 
could also include in that list 'ssn', 'foo', 'address', etc.). The next m
columns over represent different combinations of the other_columns values. 

Perf is the list of performers that supper that specific query types. The list
of eligible values for performer is 'LL','COL','IBM1','IBM2'.

In this example 'no_queries', 'r_lower', and 'r_upper'. There will be a total
of 15 queries generated, 10 of which return between 1 and 10 records and 5
that will return between 11 and 100 records. One could extend the columns 
further and have [5,101,1000] which would generate 5 additional queries that
returned between 101 and 1000 records. 

It is important to not generate fewer lines in the database than the largest
result set size. A checker module will point out the line in question and 
exit query generation. 

It tends to be easiest to create schema files within an editor like excel
rather than a simple text editor. The process of learning the distributions,
generating queries, and generating data will not happen unless provided a 
correctly formatted query_schema file. The checker module will halt with 
an error and the line of the error should the schema file be incorrectly
formatted. 

Another note on the number of queries generated, though one may specify 10
equality queries, 10 equality queries may not be output as the program 
eliminates queries that do not match the requested number of records returned
so it is worth over generating values if need be. 

The details of the schema for each of the supported types are below:


*EQ:
	Example query:
	
		43 SELECT * FROM main WHERE fname = 'LISA'
		
	Example of a csv file:
		*,,,,,,,,
		cat,sub_cat,perf,fields,"['no_queries','r_lower','r_upper']",
		EQ,eq,"['LL','COL','IBM1']","['fname','lname']","[10,1,10]","[5,11,100]","[5,101,1000]",
	
	Other_columns: (order must be maintained)
		*'no_queries' - the number of queries that return the specified result
		  set sizes
		*'r_lower' - the lower end of the range of result set size one is 
		  interested in (number of records returned by that query)
		*'r_upper' - the upper end of the range of result set size
	
	Valid sub_cats:
		*'eq'
		
	Valid fields:
		*'fname'		*'city'
		*'lname'		*'zip'
		*'ssn'			*'income'
		*'address'       	*'foo'
               *'dob'                 *'last_updated'
	
	 

*P1:
        Example queries:
	
		71 SELECT * FROM main WHERE dob = '1923-06-30' AND fname = 'ADAM' AND sex = 'Male'
		80 SELECT * FROM main WHERE military_service = 'Previous_Active_Duty' OR dob = '1972-06-16'


	Example of a csv file:
		*,,,,,,,,
		cat,sub_cat,perf,fields,"['no_queries','r_lower','r_upper','num_clauses','tm_lower','tm_upper']",,,,,
		P1,eq-and,"['LL','COL','IBM1']",['ALL'],"[1,1,10,2,1,10]","[1,11,100,2,11,100]",,,,
		P1,eq-or,"['LL','COL','IBM1']",['ALL'],"[1,1,10,2,1,10]","[1,11,100,2,11,100]",,,,
	
	Other_columns: (order must be maintained)
		
		*'no_queries' - the number of queries that return the specified result
		  set sizes for each value with that specified range. This is best 
		  illustrated through an example. Take the set no_queries = 1, 
		  r_exp_lower = 2, and r_exp_upper = 10. This would generate 1 query
		  for every range exponent in the range 2 to 10 (for a total of 9). 
		  Each of which would return generally the same number of records.
		     
		*'r_lower' - the lower end of the range of result set size one is 
		  interested in (number of records returned by that query)
		  
		*'r_upper' - the upper end of the range of result set size
		
		*'num_clauses' - the number of clauses in the query
		
		*'tm_lower'/'tm_upper' - for 'and-eq' it is the upper and lower ranges
		  of first term matches number of records, for 'eq-or' it is the 
		  total sum of records that each individual clause returns
	
	Valid sub_cats:
		*'eq-and'-simple equality conjunctions
		*'eq-or'-simple equality distjunctions
		
	Valid fields:
		*'ALL' - this denotes that all fields are eligible, performer
		  limitations are hardcoded

*P2 (non-foo fields):
	
	Example queries:
	
		600 SELECT * FROM main WHERE ssn >= '899770662'
		601 SELECT * FROM main WHERE ssn >= '899428230'

	Example of a csv file:
		*,,,,,,
		cat,sub_cat,perf,fields,"['no_queries','r_lower','r_upper','type']",,
		P2,range,"['LL','COL','IBM1']","['ssn']","[10,1,10,'less']",,,
	
	Other_columns: (order must be maintained)
		
		*'no_queries' - the number of queries that return the specified result
		  set sizes 
		     
		*'r_lower' - the lower end of the range of result set size one is 
		  interested in (number of records returned by that query)
		  
		*'r_upper' - the upper end of the range of result set size
		
		*'type' - These correspond to the sub_categories listed below
		    ~'range' - Corresponds to range, indicates a two-side range
		      query
		    ~'greater' - Corresponds to greater than, indicates a single
		      sided greater than query. 
		    ~'less' - Corresponds to less than, indicates a single sided
		      less than query
	
	Valid sub_cats:
		*'range'
		
	Valid fields:
		*'fname'         *'lname'
		*'zip' 	         *'city'
		*'last_updated'  *'ssn'
		*'income'	

*P2 (field 'foo' specific):
	
	Example queries:
	
		150 SELECT * FROM main WHERE foo BETWEEN 74 AND 184
		312 SELECT * FROM main WHERE foo >= 4207324304514021946

	Example of a csv file:
		*,,,,,,,,
		cat,sub_cat,perf,fields,"['no_queries','r_lower','r_upper','r_exp_lower','r_exp_upper','type']",,,,,
		P2,foo-range,"['LL','COL','IBM1']",['foo'],"[1,1,10,2,50,'range']",,,,
		P2,foo-greater,"['LL','COL','IBM1']",['foo'],"[1,1,10,2,50,'greater-than']",,,,
	
	Other_columns: (order must be maintained)
		
		*'no_queries' - the number of queries that return the specified result
		  set sizes for each value with that specified range. This is best 
		  illustrated through an example. Take the set no_queries = 1, 
		  r_exp_lower = 2, and r_exp_upper = 10. This would generate 1 query
		  for every range exponent in the range 2 to 10 (for a total of 9). 
		  Each of which would return generally the same number of records.
		     
		*'r_lower' - the lower end of the range of result set size one is 
		  interested in (number of records returned by that query)
		  
		*'r_upper' - the upper end of the range of result set size
		
		*'r_exp_lower' - The lower exponent for the range value. This indicates
		  the bit length of the range generated. For greater-than types of queries,
		  this is ignored beyond the effects on the aforementioned number of 
		  queries generated. 
		
		*'r_exp_lower' - The lower exponent for the range value. This indicates
		  the bit length of the range generated. For greater-than types of queries,
		  this is ignored beyond the effects on the aforementioned number of 
		  queries generated. 
		
		*'type' - These correspond to the sub_categories listed below
		    ~'range' - Corresponds to foo-range, indicates a two-side range
		      query
		    ~'greater-than' - Corresponds to foo-greater, indicates a single
		      sided greater than query. 
	
	Valid sub_cats:
		*'foo-range' - two-sided range query
		*'foo-greater' - single sided greater than query
		
	Valid fields:
		*'foo'
	
	
*P3/P4:

	Example queries:
	
		372 SELECT * FROM main WHERE CONTAINED_IN(notes4, 'characterized')
		377 SELECT * FROM main WHERE CONTAINS_STEM(notes4, 'watching')
		
	Example of a csv file:
		*,,,,,,,,
		cat,sub_cat,perf,fields,"['no_queries','r_lower','r_upper','keyword_len','type']",,,,,
		P3,word,,"['LL','COL','IBM1']"['notes4'],"[5,11,100,10,'word']",,,,,
		P4,stem,,"['LL','COL','IBM1']"['notes4'],"[5,11,100,10,'stem']",,,,,
	
	Other_columns: (order must be maintained)
		*'no_queries' - the number of queries that return the specified result
		  set sizes
		*'r_lower' - the lower end of the range of result set size one is 
		  interested in (number of records returned by that query)
		*'r_upper' - the upper end of the range of result set size
		*'keyword_len' - the length of keyword to be searched for (for example
		  'apple' would have a keyword_len of 5)
		*'type' - Corresponds to the sub_categories listed below, indicates
			which type of query to be generated
			~'word' - Corresponds to word
			~'stem' - Corresponds to stem
	
	Valid sub_cats:
		*'word' - Indicates P3 queries
		*'stem' - Indicates P4 queries
		
	Valid fields:
		*'notes1'		*'notes3'
		*'notes2'		*'notes4' 
	
	
*P6:

	Example queries:
	
		15 SELECT * FROM main WHERE fname LIKE '_ACQUELINE'
		17 SELECT * FROM main WHERE fname LIKE 'JACQUEL_NE'
		24 SELECT * FROM main WHERE fname LIKE 'JACQUELIN_'
		
	Example of a csv file:
		*,,,,,,,,
		cat,sub_cat,perf,fields,"['no_queries','r_lower','r_upper','keyword_len','type']",,,,,
		P6,wildcard,"['LL','COL','IBM1']","['fname']","[2,1,10,10,'initial-one']",,
		P6,wildcard,"['LL','COL','IBM1']","['fname']","[2,1,10,10,'middle-one']",,
		P6,wildcard,"['LL','COL','IBM1']","['fname']","[2,1,10,10,'final-one']",,
	
	Other_columns: (order must be maintained)
		*'no_queries' - the number of queries that return the specified result
		  set sizes
		*'r_lower' - the lower end of the range of result set size one is 
		  interested in (number of records returned by that query)
		*'r_upper' - the upper end of the range of result set size
		*'keyword_len' - the length of keyword to be searched for (for example
		  'app%' would have a keyword_len of 4). It includes any wildcard 
		  characters
		*'type' - Indicates which type of query to be generated, corresponds 
		  with the sub_catagories listed in the query test plan
			~'initial-one' - replaces one character with _ at beginning of word
			~'middle-one' - replaces one character with _ in middle of word
			~'final-one' - replaces one character with _ at end of word
	
	Valid sub_cats:
		*'wildcard'
		
	Valid fields:
		*'fname'
		*'lname'
		*'address'
	
		
*P7:

	Example queries:
	
		31 SELECT * FROM main WHERE address LIKE '%81 Curve Ln'
		34 SELECT * FROM main WHERE address LIKE '% Vinyard%'
		41 SELECT * FROM main WHERE address LIKE '62 Alfred C%'
	
	Example of a csv file:
		*,,,,,,,,
		cat,sub_cat,perf,fields,"['no_queries','r_lower','r_upper','keyword_len','type']",,,,,
		P7,wildcard,"['LL','COL','IBM1']","['fname']","[2,1,10,10,'initial']","[2,11,100,10,'initial']",,,,
		P7,wildcard,"['LL','COL','IBM1']","['fname']","[2,1,10,10,'both']","[2,11,100,10,'both']",,,,
		P7,wildcard,"['LL','COL','IBM1']","['fname']","[2,1,10,10,'final']","[2,11,100,10,'final']",,,,
	
	Other_columns: (order must be maintained)
		*'no_queries' - the number of queries that return the specified result
		  set sizes
		*'r_lower' - the lower end of the range of result set size one is 
		  interested in (number of records returned by that query)
		*'r_upper' - the upper end of the range of result set size
		*'keyword_len' - the length of keyword to be searched for (for example
		  'app%' would have a keyword_len of 4). It includes any wildcard 
		  characters
		*'type' - Indicates which type of query to be generated, corresponds 
		  with the sub_categories listed in the query test plan
			~'initial' - Queries of the form %stuff
			~'both' - Queries of the form %stuff%
			~'final' - Queries of the form stuff%
	
	Valid sub_cats:
		*'wildcard'
		
	Valid fields:
		*'fname'      *'notes1'      *'notes4'
		*'lname'      *'notes2'
		*'address'    *'notes3'
	

*P8/P9:
        Example queries:
	
		38 SELECT * FROM main WHERE M_OF_N(2, 3, dob = '1923-09-05', fname = 'NICK', sex = 'Male')
		92 SELECT * FROM main WHERE M_OF_N(2, 3, fname = 'RILEY', fname = 'GERARD', sex = 'Male') …
				ORDER BY M_OF_N(2, 3, fname = 'RILEY', fname = 'GERARD', sex = 'Male') DESC


	Example of a csv file:
		*,,,,,,,,,
		cat,sub_cat,perf,fields,"['no_queries','r_lower','r_upper','m','n','tm_lower','tm_upper']",,,,,
		P8,threshold,"['LL','COL','IBM1']",['ALL'],"[20,1,10,2,3,1,10]","[10,1,10,2,3,11,101]",,

	
	Other_columns: (order must be maintained)
		
		*'no_queries' - the number of queries that return the specified result
		  set sizes for each value with that specified range. This is best 
		  illustrated through an example. Take the set no_queries = 1, 
		  r_exp_lower = 2, and r_exp_upper = 10. This would generate 1 query
		  for every range exponent in the range 2 to 10 (for a total of 9). 
		  Each of which would return generally the same number of records.
		     
		*'r_lower' - the lower end of the range of result set size one is 
		  interested in (number of records returned by that query)
		  
		*'r_upper' - the upper end of the range of result set size
		
		*'m'/'n' - 
		
		*'tm_lower'/'tm_upper' - the sum of matching records for the first 
		  n-m+1 clauses
	
	Valid sub_cats:
		*'threshold'
		
	Valid fields:
		*'ALL' - this denotes that all fields are eligible, performer
		  limitations are hardcoded

	
	Note: P9 queries are the exact same queries run for P8 but with ranking syntax
	appended, so generating P8 queries will also always generate P9 queries. 

*P11:
	Example queries:
		22 SELECT * FROM main WHERE xml_value(xml,'//fname', 'DUNCAN')
		47 SELECT * FROM main WHERE xml_value(xml,'/xml/a3/b2/age',39)
		
	
	Example of a csv file:
		*,,,,,,,,
		cat,sub_cat,perf,fields,"['no_queries', 'r_lower', 'r_upper', 'path_type']",,,,,
	        P11,xml,"['LL','COL']",['xml'],"[20,1,10,'short']","[20,11,100,'short']",,,
		P11,xml,"['LL','COL']",['xml'],"[20,1,10,'full']","[20,11,100,'full']",,,
	
	Other_columns: (order must be maintained)
		*'no_queries' - the number of queries that return the specified result
		  set sizes
		*'r_lower' - the lower end of the range of result set size one is 
		  interested in (number of records returned by that query)
		*'r_upper' - the upper end of the range of result set size
		*'path_type' - Indicates which type of path the query contains:
			~'short' - Queries of the form xml//leaf_node
			~'full' - Queries of the form xml/node1/node2/leaf_node
	
	Valid sub_cats:
		*'xml'
		
	Valid fields:
		*'xml'
       
	

	 
		
	
	

	
	
